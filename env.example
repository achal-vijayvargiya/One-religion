# OpenRouter API Configuration
OPENROUTER_API_KEY=your_api_key_here

# Recommended models (all fully compatible):
OPENROUTER_MODEL=anthropic/claude-3-sonnet
# OPENROUTER_MODEL=anthropic/claude-3.5-sonnet  # Best performance
# OPENROUTER_MODEL=openai/gpt-4-turbo-preview    # Good alternative
# OPENROUTER_MODEL=openai/gpt-3.5-turbo          # Fast & cheap

# Note: Gemma models not recommended (limited system message support)
# See MODEL_COMPATIBILITY.md for full model guide

OPENROUTER_BASE_URL=https://openrouter.ai/api/v1

# Embedding Configuration
EMBEDDING_MODEL=sentence-transformers/all-MiniLM-L6-v2

# Chunking Configuration
CHUNK_SIZE=800
CHUNK_OVERLAP=200

# Vector Store Configuration
# Each book will be stored in a subdirectory: ./vector_store/bhagavad_gita/, ./vector_store/bible/, etc.
VECTOR_STORE_PATH=./vector_store

# Retrieval Configuration
TOP_K_RESULTS=5

# Agentic Grouping Configuration (for handling large documents)
# Reduce these values if you encounter context length errors
GROUPING_BATCH_SIZE=30
GROUPING_PREVIEW_LENGTH=120

# For very large documents or persistent errors, try:
# GROUPING_BATCH_SIZE=20
# GROUPING_PREVIEW_LENGTH=100

# Logging Configuration
LOG_DIR=logs
LOG_LEVEL=INFO
CONSOLE_LOG_LEVEL=INFO
LOG_MAX_BYTES=10485760
LOG_BACKUP_COUNT=5

# ============================================
# OPTIONAL: Alternative Models
# ============================================

# For larger context windows (use if processing very large documents):
# OPENROUTER_MODEL=anthropic/claude-3.5-sonnet    # 200K context
# OPENROUTER_MODEL=google/gemini-pro-1.5          # 1M context
# OPENROUTER_MODEL=openai/gpt-4-turbo-preview     # 128K context

# For better embeddings (slower but more accurate):
# EMBEDDING_MODEL=sentence-transformers/all-mpnet-base-v2
# EMBEDDING_MODEL=BAAI/bge-large-en-v1.5

# ============================================
# TROUBLESHOOTING
# ============================================

# If you get "context length exceeded" errors:
# 1. Reduce GROUPING_BATCH_SIZE to 25-30
# 2. Reduce GROUPING_PREVIEW_LENGTH to 100
# 3. Or disable agentic grouping in the UI

# For very large documents (500+ pages):
# GROUPING_BATCH_SIZE=25
# GROUPING_PREVIEW_LENGTH=100
# CHUNK_SIZE=1500

# For faster processing (at cost of accuracy):
# Disable agentic grouping in the Streamlit UI

# ============================================
# MULTI-BOOK CONFIGURATION
# ============================================

# The system supports multiple religious texts simultaneously.
# Built-in books: bhagavad_gita, bible, quran
# Book IDs use snake_case (lowercase with underscores)
# 
# To add a new book:
# 1. Place PDF in resources/<book_id>/ directory
# 2. Use the Streamlit UI or CLI to ingest:
#    - UI: Select "Add Custom Book" in the ingestion tab
#    - CLI: python pipeline.py path/to/pdf --book-id your_book_id
#
# Examples:
#   python pipeline.py resources/bible/KJV.pdf --book-id bible
#   python pipeline.py resources/quran/Quran.pdf --book-id quran
#   python pipeline.py resources/torah/Torah.pdf --book-id torah
#
# Each book gets its own vector store:
#   - vector_store/bhagavad_gita/
#   - vector_store/bible/
#   - vector_store/quran/

